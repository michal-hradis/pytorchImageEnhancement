{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "%matplotlib inline  \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import generateMotionBlurPSF\n",
    "import os\n",
    "from dataHelper import generateShakePSF, generateDefocusPSF\n",
    "\n",
    "def virtualCamera(rotXdeg, rotYdeg, rotZdeg, x, y, z, f, cropW, cropH):\n",
    "    rotX = (rotXdeg)*np.pi/180\n",
    "    rotY = (rotYdeg)*np.pi/180\n",
    "    rotZ = (rotZdeg)*np.pi/180\n",
    "\n",
    "    #Projection 2D -> 3D matrix\n",
    "    A1= np.matrix([[1, 0, -x],\n",
    "                   [0, 1, -y],\n",
    "                   [0, 0, 0 ],\n",
    "                   [0, 0, 1 ]], dtype=np.float64)\n",
    "\n",
    "    # Rotation matrices around the X,Y,Z axis\n",
    "    RX = np.matrix([[1,           0,            0, 0],\n",
    "                    [0,np.cos(rotX),-np.sin(rotX), 0],\n",
    "                    [0,np.sin(rotX),np.cos(rotX) , 0],\n",
    "                    [0,           0,            0, 1]], dtype=np.float64)\n",
    "\n",
    "    RY = np.matrix([[ np.cos(rotY), 0, np.sin(rotY), 0],\n",
    "                    [            0, 1,            0, 0],\n",
    "                    [ -np.sin(rotY), 0, np.cos(rotY), 0],\n",
    "                    [            0, 0,            0, 1]], dtype=np.float64)\n",
    "\n",
    "    RZ = np.matrix([[ np.cos(rotZ), -np.sin(rotZ), 0, 0],\n",
    "                    [ np.sin(rotZ), np.cos(rotZ), 0, 0],\n",
    "                    [            0,            0, 1, 0],\n",
    "                    [            0,            0, 0, 1]], dtype=np.float64)\n",
    "\n",
    "    #Composed rotation matrix with (RX,RY,RZ)\n",
    "    R = RX * RY * RZ\n",
    "\n",
    "    #Translation matrix on the Z axis change dist will change the height\n",
    "    T = np.matrix([[1,0,0,0],\n",
    "                   [0,1,0,0],\n",
    "                   [0,0,1,z],\n",
    "                   [0,0,0,1]], dtype=np.float64)\n",
    "\n",
    "    \n",
    "    #Camera Intrisecs matrix 3D -> 2Dnp.random.rand() * 0.6\n",
    "    A2 = np.matrix([[f, 0, cropW/2, 0],\n",
    "                   [0, f, cropH/2, 0],\n",
    "                   [0, 0,       1, 0]], dtype=np.float64)\n",
    "\n",
    "    # Final and overall transformation matrix\n",
    "    H = A2 * T * (R * A1)\n",
    "    return H\n",
    "    \n",
    "    # Apply matrix transformation\n",
    "    #cv2.warpPerspective(src, H, (w, h), dst, cv2.INTER_CUBIC)\n",
    "\n",
    "class DataSource(object):\n",
    "    def __init__(self, imageSource, path='', filterCount=1000, cropCount=1000, cropSize=128):\n",
    "        with open(imageSource, 'r') as f:\n",
    "            self.fileNames = [os.path.join(path, line.strip()) for line in f]\n",
    "\n",
    "        self.path = path\n",
    "        self.cropSize = cropSize\n",
    "        self.imgPos = 0\n",
    "        self.xRSdev = 15\n",
    "        self.yRSdev = 15\n",
    "        self.zRSdev = 6\n",
    "        self.cropPos = 0\n",
    "        self.crops = torch.cuda.ByteTensor(cropCount, 3, cropSize, cropSize)\n",
    "        \n",
    "        self.maxShakeSize = 13\n",
    "        self.maxDefocusSize = 7\n",
    "        self.maxFilterSize = self.maxShakeSize + self.maxDefocusSize + 1\n",
    "        self.filterPos = 0\n",
    "        self.filters = torch.cuda.FloatTensor(filterCount, 1, self.maxFilterSize, self.maxFilterSize)\n",
    "        \n",
    "        self.rng = np.random.RandomState(1)\n",
    "\n",
    "        \n",
    "    def updateCrops(self, cropsPerImage):\n",
    "        crops = []\n",
    "        while(len(crops) < cropsPerImage):\n",
    "            imageName = self.fileNames[self.imgPos]\n",
    "            self.imgPos = (self.imgPos + 1) % len(self.fileNames)\n",
    "            try:\n",
    "                image = cv2.imread(os.path.join(self.path, imageName))\n",
    "                if image.shape[0] * 1.25 < self.cropSize and image.shape[1] * 1.25 < self.cropSize:\n",
    "                    continue\n",
    "\n",
    "                for i in range(cropsPerImage):\n",
    "                    rotXdeg = np.random.normal() * self.xRSdev\n",
    "                    rotYdeg = np.random.normal() * self.yRSdev\n",
    "                    rotZdeg = np.random.normal() * self.zRSdev\n",
    "                    x = np.random.rand() * image.shape[1]\n",
    "                    y = np.random.rand() * image.shape[0]\n",
    "\n",
    "                    fov = (30 + np.random.rand() * 70) / 180 * np.pi\n",
    "                    baseDist = (1700/2.0) / np.tan(fov/2.0)\n",
    "                    f = baseDist\n",
    "                    baseDist *= 0.7 + np.random.rand() * 0.6\n",
    "\n",
    "                    H =  virtualCamera(rotXdeg, rotYdeg, rotZdeg, x, y, baseDist, f, self.cropSize, self.cropSize)\n",
    "                    dst = np.matrix([\n",
    "                        [0, 0, 1],\n",
    "                        [self.cropSize - 1, 0, 1],\n",
    "                        [self.cropSize - 1, self.cropSize - 1, 1],\n",
    "                        [0, self.cropSize - 1, 1]], dtype=np.float32)\n",
    "\n",
    "                    invH = np.linalg.inv(H)\n",
    "                    src = invH * dst.T\n",
    "                    src = src[0:2] / src[2:3]\n",
    "\n",
    "                    # check if inside the source image\n",
    "                    if( np.any(src < 0) or np.any(src[0] >= image.shape[1]) or np.any(src[1] >= image.shape[0])):\n",
    "                        continue\n",
    "                    \n",
    "                    # crop \n",
    "                    crop = cv2.warpPerspective(image, H, (self.cropSize, self.cropSize), flags=cv2.INTER_LINEAR)\n",
    "                    if crop.std() < 8:\n",
    "                        continue\n",
    "                    crops.append(crop)\n",
    "            except:\n",
    "                print('ERROR: While reading image \"{}\".'.format(imageName))\n",
    "\n",
    "        for i in range(len(crops)):\n",
    "            input = torch.ByteTensor(np.ascontiguousarray(crops[i].transpose(2, 0 ,1)))\n",
    "            self.crops[self.cropPos].copy_(input, async=True, broadcast=False) \n",
    "            self.cropPos = (self.cropPos + 1) % self.crops.shape[0]\n",
    "\n",
    "    def generateFilters(self, filterCount):\n",
    "        filters = []\n",
    "        while len(filters) < filterCount:\n",
    "            shakeSize = int(np.random.rand() * (self.maxShakeSize + 1)) // 2 * 2 + 1\n",
    "            defocusRadius = np.random.rand() * self.maxDefocusSize * 0.5\n",
    "\n",
    "            psf, center = generateDefocusPSF(radius=defocusRadius)\n",
    "            border = int((self.maxFilterSize - psf.shape[0]) / 2)\n",
    "            psf = np.pad(psf, [(border,border), (border,border)], mode='constant')            \n",
    "\n",
    "            if shakeSize > 0: \n",
    "                shakePsf = generateShakePSF(self.rng, resolution=shakeSize, halflife=0.75)\n",
    "                psf = cv2.filter2D(psf, -1, shakePsf)\n",
    "\n",
    "            psf /= (psf**1).sum()**1\n",
    "            filters.append(psf.reshape(1, self.maxFilterSize, self.maxFilterSize))\n",
    "                \n",
    "        for i in range(len(filters)):\n",
    "            input = torch.FloatTensor(filters[i])\n",
    "            self.filters[self.filterPos].copy_(input, async=True, broadcast=False) \n",
    "            self.filterPos = (self.filterPos + 1) % self.filters.shape[0]\n",
    "            \n",
    "    def colorManipulation(self, data):\n",
    "        self.colorSdev = 0.07\n",
    "        self.contrastSdev = 0.5\n",
    "        self.minColorSdev = 0.1\n",
    "        self.gammaSdev = 0.2\n",
    "        self.noiseSdev = 0.02\n",
    "        \n",
    "        # color and contrast\n",
    "        colorCoef = torch.cuda.FloatTensor(data.shape[0], data.shape[1], 1, 1).normal_(std=self.colorSdev)\n",
    "        contrast = torch.cuda.FloatTensor(data.shape[0], 1, 1, 1).normal_(std=self.contrastSdev).abs_()\n",
    "        data *= torch.cuda.FloatTensor([2.0]).pow(colorCoef -contrast)\n",
    "\n",
    "        # additive color\n",
    "        data += torch.cuda.FloatTensor(data.shape[0], 1, 1, 1).normal_(std=self.minColorSdev).abs_()\n",
    "        \n",
    "        # noise\n",
    "        noiseSdev = torch.cuda.FloatTensor(data.shape[0], 1, 1, 1).normal_(std=self.noiseSdev).abs_()\n",
    "        data += torch.cuda.FloatTensor(*data.shape).normal_() * noiseSdev\n",
    "        data.clamp_(0, 1.0)\n",
    "\n",
    "        # gamma\n",
    "        gamma = torch.cuda.FloatTensor(data.shape[0], 1, 1, 1).normal_(std=self.gammaSdev)\n",
    "        gamma = torch.cuda.FloatTensor([2.0]).pow(gamma)\n",
    "        data.pow_(gamma)\n",
    "        \n",
    "        return data\n",
    "\n",
    "    def getBatch(self, count=32):\n",
    "        \n",
    "        cropIdx = np.random.choice(self.crops.shape[0], count)\n",
    "\n",
    "        tmp = np.random.choice(self.filters.shape[0], count)\n",
    "        filterIdx = []\n",
    "        for v in tmp:\n",
    "            filterIdx.append(v)\n",
    "            filterIdx.append(v)\n",
    "            filterIdx.append(v)\n",
    "        \n",
    "        cropIdx = torch.cuda.LongTensor(cropIdx)\n",
    "        \n",
    "        filterIdx = torch.cuda.LongTensor(filterIdx)\n",
    "        \n",
    "        crops = self.crops[cropIdx].type(torch.cuda.FloatTensor) / 256.0\n",
    "        filters = self.filters[filterIdx].contiguous()\n",
    "        \n",
    "        vImg = Variable(crops.view(1, crops.shape[0]*3, crops.shape[2], crops.shape[3]))\n",
    "        vFilt = Variable(filters.view(crops.shape[0]*3, 1, filters.shape[2], filters.shape[3]))\n",
    "        blurred = F.conv2d(vImg, vFilt, bias=None, groups=crops.shape[0]*3)\n",
    "        \n",
    "        \n",
    "        blurred = blurred.data\n",
    "        blurred = blurred.view(crops.shape[0], 3, blurred.shape[2], blurred.shape[3])\n",
    "        \n",
    "        self.colorManipulation(blurred)\n",
    "        #crops = crops * 0.8  + 0.1\n",
    "\n",
    "        return crops, blurred, filters\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filterCount=20000\n",
    "cropCount=20000\n",
    "#del dataSource\n",
    "dataSource = DataSource(\n",
    "    '/mnt/matylda1/hradis/2015-03-06_image_restoration/PDF/data/allImages.rnd', \n",
    "    '/mnt/matylda1/hradis/2015-03-06_image_restoration/PDF/data',\n",
    "    filterCount=filterCount, cropCount=cropCount, cropSize=156) \n",
    "\n",
    "cropsPerImage=40\n",
    "for i in range(5):\n",
    "    dataSource.updateCrops(cropsPerImage)\n",
    "while(dataSource.cropPos >=  4*cropsPerImage):\n",
    "    dataSource.updateCrops(cropsPerImage)\n",
    "    print(dataSource.cropPos)\n",
    "    \n",
    "dataSource.generateFilters( filterCount)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import collage\n",
    "img, blurred, psf = dataSource.getBatch(64)\n",
    "\n",
    "data = psf.cpu().numpy()\n",
    "img = collage(data[::3], normSamples=True)\n",
    "print(img.shape)\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.imshow(img[:,:,0])\n",
    "\n",
    "data = blurred.cpu().numpy()\n",
    "print('Min/max:', data.min(), data.max())\n",
    "img = collage(data)\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.imshow(img / img.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SqueezeExitationBlock(nn.Module):\n",
    "    def __init__(self, inputChannels, filterSize=3, filterCount=32):\n",
    "        super(SqueezeExitationBlock, self).__init__()\n",
    "        \n",
    "        if filterSize > 3:\n",
    "            self.layer1 = nn.Conv2d(in_channels=inputChannels, \n",
    "                                     padding=0,\n",
    "                                     out_channels=filterCount, \n",
    "                                     kernel_size=filterSize)\n",
    "        else:\n",
    "            self.layer1 = nn.Conv2d(in_channels=inputChannels, \n",
    "                                     padding=(filterSize-1)/2,\n",
    "                                     out_channels=filterCount, \n",
    "                                     kernel_size=filterSize)\n",
    "\n",
    "        self.sqeeze = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1,1)),\n",
    "            nn.Conv2d(in_channels=filterCount, \n",
    "                     out_channels=filterCount, \n",
    "                     kernel_size=1),\n",
    "            nn.Sigmoid())\n",
    "\n",
    "        self.output = nn.Sequential(\n",
    "            nn.BatchNorm2d(num_features=filterCount, momentum=0.75),\n",
    "            nn.ReLU()\n",
    "            )\n",
    "        \n",
    "    def forward(self, input_x):\n",
    "        act = self.layer1(input_x)\n",
    "        weights = self.sqeeze(act)\n",
    "        out = self.output(act * weights)\n",
    "        return out\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, inputChannels, filterSize=3, filterCount=32):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        \n",
    "        if filterSize > 3:\n",
    "            self.layer1 = nn.Conv2d(in_channels=inputChannels, \n",
    "                                     padding=0,\n",
    "                                     out_channels=filterCount, \n",
    "                                     kernel_size=filterSize)\n",
    "        else:\n",
    "            self.layer1 = nn.Conv2d(in_channels=inputChannels, \n",
    "                                     padding=(filterSize-1)/2,\n",
    "                                     out_channels=filterCount, \n",
    "                                     kernel_size=filterSize)\n",
    "\n",
    "\n",
    "        self.output = nn.Sequential(\n",
    "            nn.BatchNorm2d(num_features=filterCount, momentum=0.75),\n",
    "            nn.ReLU()\n",
    "            )\n",
    "        \n",
    "    def forward(self, input_x):\n",
    "        act = self.layer1(input_x)\n",
    "        out = self.output(act)\n",
    "        return out\n",
    "    \n",
    "class LinearBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, inputChannels, filterSizes=[3, 3], filterCounts=[32, 32]):\n",
    "        super(LinearBlock, self).__init__()\n",
    "        \n",
    "        lastChannels = inputChannels\n",
    "        convLayers = []\n",
    "        for fSize, fCount in zip(filterSizes, filterCounts):\n",
    "            #if inputChannels > 6:\n",
    "            #    convLayers.append(torch.nn.Dropout2d(p=0.2))\n",
    "            convLayers.append(\n",
    "                ConvBlock(inputChannels=lastChannels, \n",
    "                                      filterCount=fCount, \n",
    "                                      filterSize=fSize))\n",
    "            lastChannels = fCount\n",
    "\n",
    "        self.convLayers = nn.Sequential(*convLayers)\n",
    "        \n",
    "    def forward(self, input_x):\n",
    "        return self.convLayers(input_x)\n",
    "\n",
    "class AggregationNet(nn.Module):\n",
    "    def __init__(self, inputChannels):\n",
    "        super(AggregationNet, self).__init__()\n",
    "        self.compressBlockInfo = [(2, 16), (2, 24), (2, 32)]        \n",
    "        self.compressBlocks = torch.nn.ModuleList()\n",
    "   \n",
    "        filterSizes = [13, 1, 3]    \n",
    "        filterCounts = [32, 32, 16]\n",
    "        self.compressBlocks.append(LinearBlock(inputChannels, filterSizes, filterCounts))\n",
    "        lastChannels = filterCounts[-1]\n",
    "\n",
    "        for layerCount, filterCount in self.compressBlockInfo[1:]:\n",
    "            filterSizes = [3 for i in range(layerCount)]    \n",
    "            filterCounts = [filterCount for i in range(layerCount)]\n",
    "            self.compressBlocks.append(LinearBlock(lastChannels, filterSizes, filterCounts))\n",
    "            lastChannels = filterCount\n",
    "        \n",
    "        self.outLayers = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=lastChannels, out_channels=256, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1,1)),\n",
    "            nn.Conv2d(in_channels=256, out_channels=128, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            );\n",
    "        \n",
    "    def forward(self, input_x):\n",
    "        x = input_x\n",
    "        for i, module in enumerate(self.compressBlocks):\n",
    "            x = module(x)\n",
    "            x = F.max_pool2d(x, 2, stride=2)\n",
    "        x = self.outLayers(x)\n",
    "            \n",
    "        return x\n",
    "    \n",
    "class HourGlassNet(nn.Module):\n",
    "\n",
    "    def __init__(self, inputChannels):\n",
    "        super(HourGlassNet, self).__init__()\n",
    "        \n",
    "        self.aggregation = AggregationNet(inputChannels)\n",
    "        \n",
    "        \n",
    "        self.compressBlockInfo = [(2, 24), (2, 32), (3, 64)]\n",
    "        self.decompressBlockInfo = [(2, 32), (2, 24)]\n",
    "        self.compressBlocks = torch.nn.ModuleList()\n",
    "        self.decompressBlocks = torch.nn.ModuleList()\n",
    "\n",
    "\n",
    "        self.blockMultipliers = torch.nn.ModuleList()\n",
    "        self.blockBN = torch.nn.ModuleList()\n",
    "        for fCount in [j[1] for j in self.compressBlockInfo + self.decompressBlockInfo]:\n",
    "            self.blockMultipliers.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(in_channels=128, out_channels=fCount, kernel_size=1),\n",
    "                    nn.Softmax2d(),\n",
    "                )\n",
    "            )\n",
    "            self.blockBN.append(\n",
    "                 nn.BatchNorm2d(num_features=fCount, momentum=0.75))\n",
    "\n",
    "        lastChannels = inputChannels\n",
    "\n",
    "        filterSizes = [13, 1, 3]    \n",
    "        filterCounts = [64, 32, 24]\n",
    "        self.compressBlocks.append(LinearBlock(lastChannels, filterSizes, filterCounts))\n",
    "        lastChannels = filterCounts[-1]\n",
    "\n",
    "        for layerCount, filterCount in self.compressBlockInfo[1:]:\n",
    "            filterSizes = [3 for i in range(layerCount)]    \n",
    "            filterCounts = [filterCount for i in range(layerCount)]\n",
    "            self.compressBlocks.append(LinearBlock(lastChannels, filterSizes, filterCounts))\n",
    "            lastChannels = filterCount\n",
    "\n",
    "        compressFilterCounts = [fc for lc, fc in self.compressBlockInfo[:-1]]\n",
    "\n",
    "        for compressFC, (layerCount, filterCount) in zip(compressFilterCounts[::-1], self.decompressBlockInfo):\n",
    "            filterSizes = [3 for i in range(layerCount)]    \n",
    "            filterCounts = [filterCount for i in range(layerCount)]\n",
    "            self.decompressBlocks.append(LinearBlock(lastChannels + compressFC, filterSizes, filterCounts))\n",
    "            lastChannels = filterCount\n",
    "        \n",
    "        \n",
    "        fSize = 3\n",
    "        self.outputLayer = nn.Conv2d(in_channels=lastChannels, \n",
    "                                          padding=(fSize-1)/2,\n",
    "                                          out_channels=inputChannels, \n",
    "                                          kernel_size=fSize)\n",
    "        \n",
    "    def forward(self, input_x):\n",
    "        agg = self.aggregation(input_x)\n",
    "        blockMultipliers = list(self.blockMultipliers)\n",
    "        blockBN = list(self.blockBN)\n",
    "        \n",
    "        x = input_x\n",
    "        compressStages = []\n",
    "        for i, module in enumerate(self.compressBlocks):\n",
    "            x = module(x)\n",
    "            x = blockBN[0](x * blockMultipliers[0](agg))\n",
    "            blockMultipliers = blockMultipliers[1:]\n",
    "            blockBN = blockBN[1:]\n",
    "            if i < len(self.compressBlocks) -1:\n",
    "                compressStages.append(x)\n",
    "                x = F.max_pool2d(x, 2, stride=2)\n",
    "        \n",
    "        for module, bypass in zip(self.decompressBlocks, compressStages[::-1]):\n",
    "            x = F.upsample_nearest(x, scale_factor=2)\n",
    "            x = torch.cat((x, bypass), dim=1)\n",
    "            x = module(x)\n",
    "            x = blockBN[0](x * blockMultipliers[0](agg))\n",
    "            blockMultipliers = blockMultipliers[1:]\n",
    "            blockBN = blockBN[1:]\n",
    "            \n",
    "        x = self.outputLayer(x)\n",
    "        #x = F.sigmoid(x)        \n",
    "        return x             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "net = HourGlassNet(3)\n",
    "print(net)\n",
    "net.cuda()\n",
    "\n",
    "lossHistory = []\n",
    "lossPositions = []\n",
    "iteration = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "lossHistory = []\n",
    "lossPositions = []\n",
    "iteration = 0\n",
    "\n",
    "criterion = torch.nn.MSELoss().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = 0\n",
    "dataAcc = 0\n",
    "iterationAcc = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "def updateAll(source):\n",
    "    source.stop = False\n",
    "    while(not source.stop):\n",
    "        source.generateFilters(20)\n",
    "        source.updateCrops(20)\n",
    "        time.sleep(0.3)\n",
    "        \n",
    "from threading import Thread\n",
    "updateThread = Thread(target=partial(updateAll, dataSource))\n",
    "updateThread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continueIt = 0\n",
    "if continueIt:\n",
    "    iteration = continueIt\n",
    "    data = np.load('loss_2.npz')\n",
    "    lossPositions = data['arr_0'].tolist()\n",
    "    lossHistory = data['arr_1'].tolist()\n",
    "    d = torch.load('model_{:06d}.mod'.format(iteration))\n",
    "    net.load_state_dict(d)\n",
    "\n",
    "net = net.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batchSize = 32\n",
    "viewStep = 500\n",
    "\n",
    "print(iteration, dataSource.filterPos, dataSource.cropPos)\n",
    "t1 = time.time()\n",
    "lastIteration = iterationAcc\n",
    "for i in range(1000000):\n",
    "    iteration += 1\n",
    "    optimizer.zero_grad()\n",
    "    sharp, blurred, psf = dataSource.getBatch(batchSize)\n",
    "    sharp = Variable(sharp)\n",
    "    blurred = Variable(blurred)\n",
    "    out = net(blurred)\n",
    "    b = ((sharp.data.shape[2] - out.data.shape[2])/2, \n",
    "              (sharp.data.shape[3] - out.data.shape[3])/2)\n",
    "    loss = criterion(out, sharp[:,:,b[0]:-b[0],b[1]:-b[1]])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    iterationAcc += 1\n",
    "    acc += loss.data[0]\n",
    "\n",
    "    b2 = ((sharp.data.shape[2] - blurred.data.shape[2])/2, \n",
    "              (sharp.data.shape[3] - blurred.data.shape[3])/2)\n",
    "    dataLoss = criterion(blurred, sharp[:, :, b2[0]:-b2[0], b2[1]:-b2[1]])\n",
    "    dataAcc += dataLoss.data[0]\n",
    "\n",
    "    if iteration % viewStep == viewStep-1 and iterationAcc > viewStep / 3:\n",
    "        elapsed = time.time() - t1\n",
    "        t1 = time.time()\n",
    "        print('Iteration:', iteration, dataSource.filterPos, dataSource.cropPos)\n",
    "        print('Img per second: ', iterationAcc * batchSize / elapsed)\n",
    "        print('Batch per second: ', iterationAcc / elapsed)\n",
    "        print('Elapsed time: ', elapsed)\n",
    "\n",
    "        sharp, blurred, psf = dataSource.getBatch(batchSize)\n",
    "        sharp = Variable(sharp)\n",
    "        blurred = Variable(blurred)\n",
    "\n",
    "        acc /= iterationAcc\n",
    "        dataAcc /= iterationAcc\n",
    "        print('Errors:', acc, dataAcc)\n",
    "        lossHistory.append(acc) \n",
    "        lossPositions.append(iteration)\n",
    "        iterationAcc = 0\n",
    "        acc = 0\n",
    "        dataAcc = 0 \n",
    "        vizBatchSize = 16\n",
    "\n",
    "        net = net.eval()\n",
    "        out = net(blurred)\n",
    "        net = net.train()\n",
    "        \n",
    "        out = out.data\n",
    "        out.clamp_(0, 1.0)\n",
    "        \n",
    "        out = out.cpu().numpy()\n",
    "        print(out.min(), out.max())\n",
    "        \n",
    "        fig = plt.figure(figsize=(14, 10), dpi=80, facecolor='w', edgecolor='k')\n",
    "        plt.subplot(1, 2, 1)\n",
    "        recColl = collage(out)\n",
    "        plt.imshow(recColl[:512,:512,::-1])\n",
    "        \n",
    "        b = (blurred.data.shape[2] - out.shape[2]) / 2\n",
    "        if b > 0:\n",
    "            blurred = blurred[:, :, b:-b, b:-b]\n",
    "        blurred = blurred.data.cpu().numpy()\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        blurColl = collage(blurred)\n",
    "        plt.imshow(blurColl[:512,:512,::-1])\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "        plt.subplot(1, 1, 1)\n",
    "\n",
    "        plt.semilogy(lossPositions, lossHistory)\n",
    "        plt.show()\n",
    "        np.savez('loss_2.npz', lossPositions, lossHistory)\n",
    "        cv2.imwrite('{:06d}_blur.png'.format(iteration), blurColl*256)\n",
    "        cv2.imwrite('{:06d}_rec.png'.format(iteration), recColl*256)\n",
    "        torch.save(net.state_dict(), 'model_{:06d}.mod'.format(iteration))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = net.eval()\n",
    "%timeit out = net(blurred)\n",
    "net = net.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lossPositions[1:], lossHistory[1:])\n",
    "print(iteration, iterationAcc)\n",
    "l0 = net.layers[0]\n",
    "l0 = l0.weight.data.cpu().numpy()\n",
    "l0 -= l0.min()\n",
    "l0 /= l0.max()\n",
    "\n",
    "img = collage(l0)\n",
    "#plt.imshow(img[:,:,::-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = net.psfEmbed.weight.data.cpu()\n",
    "data -= data.min()\n",
    "data /= data.max()\n",
    "data = data.numpy()\n",
    "data = data.reshape(data.shape[0]*net.filterCount, 3, net.filterSize, net.filterSize)\n",
    "print(data.shape)\n",
    "img = collage(data)\n",
    "plt.imshow(img)\n",
    "plt.draw()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = out.data.cpu().numpy()\n",
    "img = collage(res)\n",
    "print(img.shape, res.shape, data.shape)\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(img/255.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psfIdx = Variable(Tidx.cuda())\n",
    "f = net.psfEmbed(psfIdx)\n",
    "f = f.view(f.data.shape[0], f.data.shape[2])\n",
    "f = F.relu(net.psfFC1(f))\n",
    "f = net.psfFC2(f)\n",
    "f = F.softmax(f)\n",
    "f = f.view(128, 3, net.filterSize, net.filterSize)\n",
    "\n",
    "res = f.data.cpu().numpy()\n",
    "plt.subplot(1, 2, 1)\n",
    "img = collage(res[:16,:,:,:], True)\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSource."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
